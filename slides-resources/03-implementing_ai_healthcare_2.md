---
marp: true
theme: uncover
_class: invert
paginate: true

---
<style>
    p {
        text-align: left;
        font-size: 30px
    }
    ul {
        margin: 0;
        font-size: 30px;
    }
    table {
        font-size: 30px;
    }
    ol {
        margin: 0;
        font-size: 30px;
    }
    figcaption {
        font-size: 15px;
        text-align: center;
    }
</style>

# **Lecture 3: Implementing AI in Healthcare (part 2)**
```console
Data Sciences Institute
Topics in Deep Learning
Instructor: Erik Drysdale
TA: Jenny Du
```

---


- We should stop training radiologists now. It's just completely obvious that within five years, deep learning is going to do better than radiologists.
    - [Geoff Hinton (2016)](https://www.youtube.com/watch?v=2HMPRXstSvQ), the “godfather of AI”

<p align="center"><img src="images/shortage_canada.png" style="width: 500px"></p>
<p align="center"><img src="images/shortage_global.png" style="width: 500px"></p>

<!-- Question: What happened? Was Geoff Hinton wrong in assuming radiologists would be superfluous? Why do we now have a shortage? -->

---
##### **Lecture Outline**

  - Bias (ethical)
  - Bias (statistical)
  - Addressing risk
  - Generalization challenges

---

##### **Introduction**
- The integration of AI in healthcare has great potential for improving patient care, but it is not without challenges.
- This presentation will delve into key pitfalls: bias, risk, and generalization, associated with AI in healthcare.
  
---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Bias (ethical)`

---
- Bias in AI refers to the systematic and unfair discrimination or favoritism in the outcomes produced by artificial intelligence systems, algorithms, or models.
- In healthcare it may lead to unequal access to healthcare, inaccurate diagnoses, or disparities in treatment recommendations based on various factors.

---
##### **Bias is inherent in medical practice**

<img src="images/bias_language.png" style="width: 600px">

[Source](https://youtu.be/ZTiLwk3VO3s): Artifical Intelligence and Nursing - NPAO 2021

<!-- Question: What should we do here? Should we include language as a variable in our model? Does anyone thing we should use language as a clinical criteria for receiving a DI order? -->

---
##### **And is invisible to the human eye**


<img src="images/perf_race_noise.png" style="width: 700px">

Source: [Gichoya et. al (2022)](https://www.thelancet.com/journals/landig/article/PIIS2589-7500%2822%2900063-2/fulltext)

<!-- Question: What do people think is going on here? How is this possible? -->

---
##### **Which means it will be inherent in model inference**

<img src="images/fpr_subgroup.png" style="width: 800px">

Source: [Zhang et. al (2023)](https://mit-serc.pubpub.org/pub/algorithmic-chest/release/2)

---
##### **Which means it will be inherent in model inference**

![](images/token_race.png)

Token completion generated by SciBERT (see [Zhang et. al (2020)](https://dl.acm.org/doi/pdf/10.1145/3368555.3384448))


---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Bias (statistical)`

<!-- Question: What is the definition of "bias" from a statisticians perspective? -->

---
##### **Bigger is not always better**

- If you wanted to know a proportion (e.g. % who will vote for a president, true positive rate, etc), do you want 400 truly random samples, or 2.3 million samples where there's a 0.5% bias against reporting for one group?
    - Answer: n=400 (source [Meng (2018)](https://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf))
- Representativeness is key!

<!-- Question: What do we mean here by better? What fundamental issue are we talking about here that applies to both statistics and ML? -->


---
##### **Test set structure**

- It's very important to create a test set that (most) closely resembles prospective deployment

<img src="images/bias_data_split.png" style="width: 500px">

---
##### **Test set structure**

<img src="images/test_set_structure.png" style="width: 800px">

Source: [Nestor et. al (2021)](https://www.medrxiv.org/content/10.1101/2021.05.11.21257052v1.full.pdf)

---
<!--_color: white -->
<!--_backgroundColor: green -->
## `Breakout #1`
#### Why would would we expect prospective test set performance to be worse on average than a random split?

---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Types of Bias`

---
##### **Selection Bias**
- Selection bias is associated with the manner in which the data used for training or evaluation is collected.
- It arises when the data collection process favors certain groups or circumstances over others.
- Selection bias can introduce systemic bias into the dataset (i.e. non-representativeness).

---
##### **Labeling Bias**
- Can arise when the labels assigned to training data reflect existing prejudices or stereotypes
- Can also occur during the annotation or labeling of data points.

<img src="images/label_bias.png" style="width: 800px">

Source: [Obermeyer et. al (2019)](https://www.science.org/doi/10.1126/science.aax2342)

---
##### **Algorithmic Bias**
- Algorithmic bias relates to inherent biases in the design or structure of the AI algorithms themselves.
- It can result from the way features are selected, weighted, or processed during decision-making ([example](https://arxiv.org/pdf/1602.04938.pdf): Ribeiro et. al (2016))

<img src="images/husky_wolf.png" style="width: 350px">

<!-- Question: What is going on here? Why is the explainer points to the background of the image? -->


---
<!--_color: white -->
<!--_backgroundColor: green -->
## `Breakout #2`
#### What issues would arise if we trained a melanoma classier on these sorts of images?

<a href="https://www.sciencedirect.com/science/article/pii/S0022202X18322930?via%3Dihub=">
    <img src="images/melanoma_rulers.png" style="display: block; margin-left: auto; margin-right: auto; width: 400px">
</a>


---
##### **Reinforcement Bias**
- Reinforcement bias emerges from the interactions between AI systems and users.
- It results from AI systems learning from user feedback and behavior.
- If users exhibit biased behavior, the AI may reinforce these biases in its responses.
    - See Hidden Risks of Machine Learning Applied to Healthcare ([Adam et. al (2020)](http://proceedings.mlr.press/v126/adam20a/adam20a.pdf))

<a href="images/animated_bias.gif">
    <img src="images/animated_bias.gif" style="width: 350px">
</a>

<!-- Question: What are some examples in the RW (doesn't have to be healthcare) where this would be the case?  -->


---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Addressing Bias`

---
##### **Diverse and Inclusive Data Collection**
- Collect diverse and representative data to train AI models.
- Ensure that data includes various demographic, geographic, and socio-economic factors.
- Pay special attention to underrepresented or marginalized groups to avoid skewed or biased training data.

---
##### **Data Preprocessing and Cleaning**
 - Implement rigorous data preprocessing techniques to identify and mitigate bias in training data.
 - Remove or re-weight biased or sensitive attributes from the dataset to minimize the potential for bias to be learned by the AI system.

---
##### **Fairness and Bias Audits**
 - Conduct regular fairness audits of AI models to detect and quantify bias.
 - Use specialized tools and metrics (e.g., disparate impact, equal opportunity) to assess the fairness of model outcomes across different groups.

---
##### **Transparency and Explainability**
- Make AI models more transparent and interpretable to understand the factors influencing their decisions.
- Implement techniques like explainable AI (XAI) to provide insights into model behavior and allow for the identification and rectification of bias.

---
##### **Continuous Monitoring and Feedback Loop**
- Establish a feedback loop for continuous monitoring and improvement of AI systems' fairness.
- Collect feedback from users and impacted communities to identify and address bias issues as they arise, making ongoing refinements to models and data.

---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Risk`

---

- Risk in AI refers to the potential negative consequences or uncertainties associated with the development, deployment, and use of artificial intelligence systems. 

---

<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Examples of Healthcare AI Risks`

---

##### **Data Breaches**
- Healthcare data is particularly sensitive.
- Breaches can expose patient information, leading to privacy violations and legal consequences.

---

##### **Incorrect Diagnoses**
- AI systems that assist in diagnostics could potentially make incorrect diagnoses, leading to improper treatment and harm to patients.

---

##### **Legal Liabilities**
- Healthcare providers using AI systems face legal risks if the technology leads to patient harm, including malpractice claims.

---

##### **Ethical Concerns**
- Decisions about patient care based on AI could raise ethical issues, especially regarding consent, transparency, and the prioritization of healthcare resources.

---

<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Addressing Risk`

---

##### **Robust Data Security Measures**
- Implement strong data protection practices like encryption, access controls, and regular security audits.
- Ensure compliance with regulations like General Data Protection Regulation (GDPR) and Health Insurance Portability and Accountability Act (HIPAA) to safeguard sensitive health data.

---

##### **Transparent and Explainable AI**
- Develop AI systems that are understandable and transparent, allowing healthcare professionals to grasp how AI decisions are made.
- Can help in understanding AI model's decision-making process, providing justification for the decisions made, and identifying biases.

---

##### **Ethical AI Development and Use**
- Adhere to ethical principles in AI development to ensure fairness, avoid bias, and respect patient autonomy and privacy.

---

##### **Rigorous Testing and Validation**
- Subject AI systems to extensive testing and validation to confirm their safety and efficacy, and that they perform as intended across diverse patient populations.
- May include clinical trials followed by continuous monitoring post-deployment.

---

##### **Legal and Regulatory Compliance**
- Ensure AI systems comply with medical, data protection, and patient rights laws.
- Make sure to adapt to legal changes.

---

<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Generalization`

---
- Generalization in AI refers to the ability of an AI system or model to perform well on new, unseen data after having been trained on a specific set of data.


---
##### **ML models are "extremely sensitive"**

- All deep learning systems can be rendered useless by adverserial attacks

<br>
<img src="images/adverserial_attack.png" style="width: 700px">

[Source](https://arxiv.org/pdf/1412.6572.pdf): Goodfellow et. al (2015)

<!-- Question: Why are DNNs especially prone to adverserial attacks? -->

---
##### **Can be easily tricked by artefacts**

- Example of CNN picking up on hospital-specific X-ray practices (source: [Zech et. al (2018)](https://journals.plos.org/plosmedicine/article/file?id=10.1371/journal.pmed.1002683&type=printable))

<br>
<img src="images/bias_artefacts.png" style="width: 550px">


---
##### **Model drift**

- After a model goes live the performance of the model will often suffer
    - Unconditional label distribution changes
    - Unconditional feature distribution changes
    - Conditional relationship b/w label and features changes

<img src="images/bias_drift.png" style="width: 350px">

[Source](http://proceedings.mlr.press/v106/nestor19a/nestor19a.pdf): Nestor et. al (2019)

<!-- Question: Which of these three factors are the most important? Which is the easiest to test for?  -->

---
##### **Overfitting vs. Underfitting**
- Good generalization requires balance between overfitting and underfitting.
- Overfitting: Model learns the training data too well & performs poorly on new data. 
- Underfitting: Model is too simple to capture the underlying structure of the data & performs poorly on training and new data.

<!-- Question: How do you know if your model is overfitting vs underfitting? -->

---

<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Importance of Generalization in Healthcare`

---

##### **Diverse Patient Populations**
- Healthcare datasets come from diverse populations with varying demographics, medical histories, and health conditions. 
- Generalization ensures that AI models can effectively handle data from varied patient groups.

---

##### ****Variability in Medical Data****
- Medical data can be highly variable (i.e., imaging data, electronic health records (EHRs), genetic information).
- Each type of data has differences in quality, format, and context.
- Generalization ensures AI models can provide reliable insights across various types of medical data.

---

##### **Changing Healthcare Practices and Knowledge**
- Healthcare is a rapidly evolving field (i.e., new treatments, diagnostic criteria, and research findings). 
- Generalization ensures AI models are better equipped to remain relevant and accurate as medical knowledge and practices evolve.

---
<!--_color: white -->
<!--_backgroundColor: #f4a534 -->
## `Addressing Generalization`

---

##### **Training Data Diversity**
- Model trained on a very diverse dataset is more likely to generalize well because it has been exposed to a wide variety of examples.

---

##### **Regularization Techniques**
- Dropout, L1/L2 regularization, and early stopping help to prevent overfitting by penalizing complexity or stopping the training process before the model starts to overfit.

---

##### **Cross-validation**
- Involves dividing the dataset into several subsets, training the model on some subsets and validating it on others. 
- Helps in assessing the model's ability to generalize across different data splits.

---

##### **Model Complexity**
- Simpler models usually underfit but more complex models usually overfit.
- Need to find the right level of complexity.

---
##### **Transfer Learning**
- Involves taking a model that has been trained on one task and adapting it to a different but related task. 
- Can help in situations where there is not enough data for training a model from scratch, leveraging the generalization capabilities learned from the original task.
